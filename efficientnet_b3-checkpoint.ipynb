{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"FBQocLLMEOkA"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport cv2\nfrom PIL import Image\nimport gc\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n!pip install torchtoolbox==0.1.4.1\nimport torchtoolbox.transform as transforms\n# from torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\n\nimport time\nimport datetime\nimport random\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n\n!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\n\nimport os \n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"scrolled":true,"id":"vYU2bLwBBOA_","outputId":"1c817d42-0ae5-46e3-d1fa-c9134b145420","execution":{"iopub.status.busy":"2022-05-26T12:28:47.147308Z","iopub.execute_input":"2022-05-26T12:28:47.147689Z","iopub.status.idle":"2022-05-26T12:29:05.286056Z","shell.execute_reply.started":"2022-05-26T12:28:47.147644Z","shell.execute_reply":"2022-05-26T12:29:05.285145Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install wandb\n!wandb login","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-26T12:29:05.289513Z","iopub.execute_input":"2022-05-26T12:29:05.289882Z","iopub.status.idle":"2022-05-26T12:29:35.883490Z","shell.execute_reply.started":"2022-05-26T12:29:05.289841Z","shell.execute_reply":"2022-05-26T12:29:35.882488Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nwandb.init(project=\"Melanoma\", entity=\"grimm0404\")","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:29:35.885087Z","iopub.execute_input":"2022-05-26T12:29:35.885460Z","iopub.status.idle":"2022-05-26T12:29:45.031683Z","shell.execute_reply.started":"2022-05-26T12:29:35.885401Z","shell.execute_reply":"2022-05-26T12:29:45.030833Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Seeds","metadata":{"id":"X-i6D7jah5KX"}},{"cell_type":"code","source":"# 设置随机种子\ndef seed_everything(seed_value):\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 1234\nseed_everything(seed)","metadata":{"id":"AJZ6Z9puh6By","execution":{"iopub.status.busy":"2022-05-26T12:29:45.033055Z","iopub.execute_input":"2022-05-26T12:29:45.033391Z","iopub.status.idle":"2022-05-26T12:29:45.098225Z","shell.execute_reply.started":"2022-05-26T12:29:45.033353Z","shell.execute_reply":"2022-05-26T12:29:45.097365Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Setting up the Device ","metadata":{"id":"EJIdZdAf9rXD"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (device)","metadata":{"id":"QwoV7i5g9zrI","outputId":"8473f840-069c-45ff-9d1f-483ddb15ee24","execution":{"iopub.status.busy":"2022-05-26T12:29:45.101672Z","iopub.execute_input":"2022-05-26T12:29:45.102053Z","iopub.status.idle":"2022-05-26T12:29:45.107360Z","shell.execute_reply.started":"2022-05-26T12:29:45.102011Z","shell.execute_reply":"2022-05-26T12:29:45.106528Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Creating Dataset","metadata":{"id":"ofellIJSiPc0"}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, img_dir, train: bool = True, transforms= None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.train = train\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.img_dir, self.df.iloc[index]['image_name'] + '.jpg')\n        #images = Image.open(img_path)\n        images = cv2.imread(img_path)\n\n        if self.transforms:\n            images = self.transforms(images)\n\n        if self.train:\n            labels = self.df.iloc[index]['target']\n            #return images, labels\n            return torch.tensor(images, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n#             return torch.tensor(images, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n\n        else:\n            #return (images)\n            return torch.tensor(images, dtype=torch.float32)\n\n    def __len__(self):\n            return len(self.df)","metadata":{"id":"LUvAW33pfTnx","execution":{"iopub.status.busy":"2022-05-26T12:29:45.110917Z","iopub.execute_input":"2022-05-26T12:29:45.111474Z","iopub.status.idle":"2022-05-26T12:29:45.123106Z","shell.execute_reply.started":"2022-05-26T12:29:45.111434Z","shell.execute_reply":"2022-05-26T12:29:45.122250Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/melanoma-external-malignant-256/train_concat.csv')\ntest_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\ntest_img_dir = '/kaggle/input/melanoma-external-malignant-256/test/test/'\ntrain_img_dir = '/kaggle/input/melanoma-external-malignant-256/train/train/'","metadata":{"scrolled":true,"id":"dchPK1V0BOBN","execution":{"iopub.status.busy":"2022-05-26T12:29:45.126307Z","iopub.execute_input":"2022-05-26T12:29:45.126634Z","iopub.status.idle":"2022-05-26T12:29:45.236137Z","shell.execute_reply.started":"2022-05-26T12:29:45.126602Z","shell.execute_reply":"2022-05-26T12:29:45.235386Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"vld_size=0.20\n\ntrain, valid = train_test_split (df, stratify=df.target, test_size = vld_size, random_state=42) \n\ntrain_df=pd.DataFrame(train)\nvalidation_df=pd.DataFrame(valid)\n\nprint(len(validation_df))\nprint(len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:29:45.238570Z","iopub.execute_input":"2022-05-26T12:29:45.239155Z","iopub.status.idle":"2022-05-26T12:29:45.284768Z","shell.execute_reply.started":"2022-05-26T12:29:45.239113Z","shell.execute_reply":"2022-05-26T12:29:45.283867Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Targets in Training and Validation sets\nTargets (in this case melanoma images) should be equally distributed","metadata":{}},{"cell_type":"code","source":"fig2 = plt.figure(figsize=(20, 5))\nax3 = fig2.add_subplot(1,2,1)\nax4 = fig2.add_subplot(1,2,2)\n\ncounts1 = train_df['target'].value_counts()\ndx = ['Benign', 'Malignant']\nax3.bar(dx, counts1)  \nax3.set_title(\"Training Set\")\nax3.legend()\n\nfor i, v in enumerate(counts1):\n    ax3.text(i-.1, \n              v/counts1[i]+200, \n              counts1[i], \n              fontsize=15,\n              )\n\ncounts2 = validation_df['target'].value_counts()\nax4.bar(dx, counts2)  \nax4.set_title(\"Validation Set\")\nax4.legend()\n\nfor i, v in enumerate(counts2):\n    ax4.text(i-.1, \n              v/counts2[i]+100, \n              counts2[i], \n              fontsize=15)\n\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:29:45.286121Z","iopub.execute_input":"2022-05-26T12:29:45.286684Z","iopub.status.idle":"2022-05-26T12:29:45.538611Z","shell.execute_reply.started":"2022-05-26T12:29:45.286643Z","shell.execute_reply":"2022-05-26T12:29:45.537668Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{}},{"cell_type":"code","source":"# Defining transforms for the training, validation, and testing sets\ntraining_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                          transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n                                          transforms.RandomHorizontalFlip(),\n                                          transforms.RandomVerticalFlip(),\n                                          transforms.ColorJitter(brightness=32. / 255.,saturation=0.5,hue=0.01),\n                                          transforms.ToTensor(),\n                                          transforms.Normalize([0.485, 0.456, 0.406], \n                                                               [0.229, 0.224, 0.225])])\n\nvalidation_transforms = transforms.Compose([transforms.Resize(256),\n                                            transforms.CenterCrop(256),\n                                            transforms.ToTensor(),\n                                            transforms.Normalize([0.485, 0.456, 0.406], \n                                                                 [0.229, 0.224, 0.225])])\n\ntesting_transforms = transforms.Compose([transforms.Resize(256),\n                                         transforms.CenterCrop(256),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize([0.485, 0.456, 0.406], \n                                                              [0.229, 0.224, 0.225])])","metadata":{"scrolled":true,"id":"UzAdvS3EBOBX","execution":{"iopub.status.busy":"2022-05-26T12:29:45.540090Z","iopub.execute_input":"2022-05-26T12:29:45.540672Z","iopub.status.idle":"2022-05-26T12:29:45.552240Z","shell.execute_reply.started":"2022-05-26T12:29:45.540630Z","shell.execute_reply":"2022-05-26T12:29:45.551291Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Loading Datasets","metadata":{"id":"UJR_kmLtiegA"}},{"cell_type":"code","source":"training_dataset = CustomDataset(df = train_df,\n                                 img_dir = train_img_dir, \n                                 train = True,\n                                 transforms = training_transforms )\n\nvalidation_dataset = CustomDataset(df = validation_df,\n                                   img_dir = train_img_dir, \n                                   train = True,\n                                   transforms = training_transforms )\n\ntesting_dataset = CustomDataset(df = test_df,\n                                img_dir = test_img_dir,\n                                train= False, \n                                transforms = testing_transforms )","metadata":{"id":"p9llHQMpmjOv","execution":{"iopub.status.busy":"2022-05-26T12:29:45.553636Z","iopub.execute_input":"2022-05-26T12:29:45.553931Z","iopub.status.idle":"2022-05-26T12:29:45.566166Z","shell.execute_reply.started":"2022-05-26T12:29:45.553890Z","shell.execute_reply":"2022-05-26T12:29:45.565251Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=32, num_workers=4, shuffle=True)\nvalidate_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=16, shuffle = False)","metadata":{"id":"EaNxLBcTi3bw","execution":{"iopub.status.busy":"2022-05-26T12:29:45.567912Z","iopub.execute_input":"2022-05-26T12:29:45.568452Z","iopub.status.idle":"2022-05-26T12:29:45.579679Z","shell.execute_reply.started":"2022-05-26T12:29:45.568394Z","shell.execute_reply":"2022-05-26T12:29:45.578654Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(len(train_loader))\nprint(len(validate_loader))\nprint(len(test_loader))","metadata":{"id":"e360v3HeBOBf","outputId":"84d22231-3021-460f-c8b9-bebe46bf6345","execution":{"iopub.status.busy":"2022-05-26T12:29:45.581619Z","iopub.execute_input":"2022-05-26T12:29:45.582002Z","iopub.status.idle":"2022-05-26T12:29:45.591930Z","shell.execute_reply.started":"2022-05-26T12:29:45.581972Z","shell.execute_reply":"2022-05-26T12:29:45.590990Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Building the Model\n\nNow that the data is ready, it's time to build the Model. I will be using a pretrained model 'Efficient Net' to get the image features and then modify it to fit the Dataset","metadata":{"id":"wtE-5PEhBOBv"}},{"cell_type":"code","source":"# class Net(nn.Module):\n#     def __init__(self, arch):\n#         super(Net, self).__init__()\n#         self.arch = arch\n#         if 'fgdf' in str(arch.__class__):\n#             self.arch.fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n#         if 'EfficientNet' in str(arch.__class__):   \n#             self.arch._fc = nn.Linear(in_features=1408, out_features=500, bias=True)\n            \n#         self.ouput = nn.Linear(500, 1)\n        \n#     def forward(self, images):\n#         \"\"\"\n#         No sigmoid in forward because we are going to use BCEWithLogitsLoss\n#         Which applies sigmoid for us when calculating a loss\n#         \"\"\"\n#         x = images\n#         features = self.arch(x)\n#         output = self.ouput(features)\n        \n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:29:45.593800Z","iopub.execute_input":"2022-05-26T12:29:45.594326Z","iopub.status.idle":"2022-05-26T12:29:45.600947Z","shell.execute_reply.started":"2022-05-26T12:29:45.594225Z","shell.execute_reply":"2022-05-26T12:29:45.599919Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.resnetmodel = EfficientNet.from_pretrained('efficientnet-b3')\n        \n        self.fc = nn.Sequential(nn.Linear(1000, 512), nn.ReLU(),\n                                  nn.Linear(512, 1))\n#                                 , nn.Sigmoid())\n        \n    def forward(self, x):\n        x = self.resnetmodel(x)\n        return self.fc(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:29:45.602970Z","iopub.execute_input":"2022-05-26T12:29:45.603254Z","iopub.status.idle":"2022-05-26T12:29:45.612194Z","shell.execute_reply.started":"2022-05-26T12:29:45.603228Z","shell.execute_reply":"2022-05-26T12:29:45.611315Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# import torchvision.models as models\n# arch = EfficientNet.from_pretrained('efficientnet-b2')\n# arch = EfficientNet.from_pretrained('efficientnet-b3')\n# model = Net(arch=arch) \nmodel = Model()\n# model = models.resnet152(pretrained=True)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:29:45.613492Z","iopub.execute_input":"2022-05-26T12:29:45.613892Z","iopub.status.idle":"2022-05-26T12:29:51.611520Z","shell.execute_reply.started":"2022-05-26T12:29:45.613855Z","shell.execute_reply":"2022-05-26T12:29:51.610722Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"id":"nxXejLkPTJdz"}},{"cell_type":"code","source":"# Empty variable to be stored with best validation accuracy\nbest_val = 0\n\n# Path and filename to save model to\nmodel_path = f'melanoma_model_{best_val}.pth'  \n\n# Number of Epochs\nepochs = 10\n\n# Early stopping if no change in accurancy\nes_patience = 3\n\ncriterion = nn.BCEWithLogitsLoss()\n# criterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.0005) \n\n# Scheduler\nscheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=1, verbose=True, factor=0.2)","metadata":{"scrolled":true,"id":"FbgDZOZjBOCG","execution":{"iopub.status.busy":"2022-05-26T12:29:51.633534Z","iopub.execute_input":"2022-05-26T12:29:51.634157Z","iopub.status.idle":"2022-05-26T12:29:51.643809Z","shell.execute_reply.started":"2022-05-26T12:29:51.634120Z","shell.execute_reply":"2022-05-26T12:29:51.642821Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Trainning the Model","metadata":{}},{"cell_type":"code","source":"#from workspace_utils import active_session -> this can be used so that the session remains on and not disconnect\n\n  \nloss_history=[]  \ntrain_acc_history=[]  \nval_loss_history=[]  \nval_acc_history=[] \nval_auc_history=[]\n\n    \npatience = es_patience\nTotal_start_time = time.time()  \nmodel.to(device)\n\nfor e in range(epochs):\n    \n    start_time = time.time()\n    correct = 0\n    running_loss = 0\n    model.train()\n    \n    for images, labels in train_loader:\n        \n        \n        images, labels = images.to(device), labels.to(device)\n#         images = images.view(images.size(0), -1)\n        \n        optimizer.zero_grad()\n        \n        output = model(images) \n        loss = criterion(output, labels.view(-1,1))\n        loss.backward()\n        optimizer.step()\n        \n        # Training loss\n        running_loss += loss.item()\n\n        # Number of correct training predictions and training accuracy\n        train_preds = torch.round(torch.sigmoid(output))\n            \n        correct += (train_preds.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n                        \n    train_acc = correct / len(train_df)\n        \n        \n    #switching to validation:        \n    model.eval()\n    preds=[]            \n    # Turning off gradients for validation, saves memory and computations\n    with torch.no_grad():\n        \n        val_loss = 0\n        val_correct = 0\n    \n        for val_images, val_labels in validate_loader:\n         \n        \n            val_images, val_labels = val_images.to(device), val_labels.to(device)\n\n        \n            val_output = model(val_images)\n            val_loss += (criterion(val_output, val_labels.view(-1,1))).item() \n            val_pred = torch.sigmoid(val_output)\n            \n            preds.append(val_pred.cpu())\n        pred=np.vstack(preds).ravel()\n           \n#         val_accuracy = accuracy_score(train_df['target'].values, torch.round(pred2))\n        val_auc_score = roc_auc_score(validation_df['target'].values, pred)\n            \n        training_time = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n            \n        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n              \"Training Loss: {:.3f}\".format(running_loss/len(train_loader)),\n              wandb.log({\"train_loss\": running_loss/len(train_loader)}),\n              \n              \"Training Accuracy: {:.3f}\".format(train_acc),\n              wandb.log({\"train_acc\": train_acc}),\n              \n              \"Validation Loss: {:.3f}\".format(val_loss/len(validate_loader)),\n              wandb.log({\"val_loss\": val_loss/len(validate_loader)}),\n              \n#               \"Validation Accuracy: {:.3f}\".format(val_accuracy),\n#               wandb.log({\"val_acc\": val_accuracy}),\n              \n              \"Validation AUC Score: {:.3f}\".format(val_auc_score),\n              wandb.log({\"val_auc_score\": val_auc_score}),\n              \"Training Time: {}\".format( training_time))\n            \n          \n        scheduler.step(val_auc_score)\n                \n        if val_auc_score >= best_val:\n            best_val = val_auc_score\n            patience = es_patience  # Resetting patience since we have new best validation accuracy\n            torch.save(model, model_path)  # Saving current best model\n        else:\n            patience -= 1\n            if patience == 0:\n                print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n                break\n        \n    loss_history.append(running_loss)  \n    train_acc_history.append(train_acc)    \n    val_loss_history.append(val_loss)  \n    #val_acc_history.append(val_accuracy)\n    val_auc_history.append(val_auc_score)\n    \n\ntotal_training_time = str(datetime.timedelta(seconds=time.time() - Total_start_time  ))[:7]                  \nprint(\"Total Training Time: {}\".format(total_training_time))\n                  \n              ","metadata":{"id":"34YYRItSBOCU","outputId":"331ab0b5-6baa-4471-9259-be514e7e8e9e","execution":{"iopub.status.busy":"2022-05-26T12:29:51.645287Z","iopub.execute_input":"2022-05-26T12:29:51.645725Z","iopub.status.idle":"2022-05-26T13:49:43.337591Z","shell.execute_reply.started":"2022-05-26T12:29:51.645686Z","shell.execute_reply":"2022-05-26T13:49:43.336743Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Ploting losses and accuracies","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 5))\nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\n\nax1.plot(loss_history, label= 'Training Loss')  \nax1.plot(val_loss_history,label='Validation Loss')\nax1.set_title(\"Losses\")\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss')\nax1.legend()\n\nax2.plot(train_acc_history,label='Training accuracy')  \n#ax2.plot(val_acc_history,label='Validation accuracy')\nax2.plot(val_auc_history,label='Validation AUC Score')\nax2.set_title(\"Accuracies\")\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Accuracy')\nax2.legend()\n\nplt.savefig('d02.jpg')\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:49:43.339360Z","iopub.execute_input":"2022-05-26T13:49:43.339730Z","iopub.status.idle":"2022-05-26T13:49:43.998850Z","shell.execute_reply.started":"2022-05-26T13:49:43.339698Z","shell.execute_reply":"2022-05-26T13:49:43.996537Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Deleting unnecessary variables and cleaning up Cache","metadata":{}},{"cell_type":"code","source":"del training_dataset, validation_dataset, train_loader, validate_loader, images, val_images, val_labels\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:49:44.002861Z","iopub.execute_input":"2022-05-26T13:49:44.004934Z","iopub.status.idle":"2022-05-26T13:49:44.252379Z","shell.execute_reply.started":"2022-05-26T13:49:44.004893Z","shell.execute_reply":"2022-05-26T13:49:44.251373Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"tnqLx-40BOCg"}},{"cell_type":"code","source":"test_df['target']= np.zeros((len(test_df), 1))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:49:44.256673Z","iopub.execute_input":"2022-05-26T13:49:44.258777Z","iopub.status.idle":"2022-05-26T13:49:44.266421Z","shell.execute_reply.started":"2022-05-26T13:49:44.258737Z","shell.execute_reply":"2022-05-26T13:49:44.265485Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_labels = torch.tensor(test_df['target'], dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:49:44.270655Z","iopub.execute_input":"2022-05-26T13:49:44.273086Z","iopub.status.idle":"2022-05-26T13:49:44.283030Z","shell.execute_reply.started":"2022-05-26T13:49:44.273046Z","shell.execute_reply":"2022-05-26T13:49:44.282084Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = torch.load(model_path)\nmodel.eval()\nmodel.to(device)\ntest_preds=[]\nwith torch.no_grad():\n    \n    for f, (test_images) in enumerate(test_loader):\n        \n        \n        test_images, test_labels = test_images.to(device), test_labels.to(device)\n        \n        \n        test_output = model(test_images)\n        test_pred = torch.sigmoid(test_output)\n            \n        test_preds.append(test_pred.cpu())\n        \n    test_pred=np.vstack(test_preds).ravel()\n    test_pred2 = torch.tensor(test_pred)\n    test_accuracy = accuracy_score(test_labels.cpu(), torch.round(test_pred2))\n      \n        \n    \nprint(\"Test Accuracy: {}\".format(test_accuracy))    \n        ","metadata":{"scrolled":true,"id":"l8ox_qGkBOCh","execution":{"iopub.status.busy":"2022-05-26T13:49:44.287140Z","iopub.execute_input":"2022-05-26T13:49:44.289205Z","iopub.status.idle":"2022-05-26T13:52:18.757644Z","shell.execute_reply.started":"2022-05-26T13:49:44.289166Z","shell.execute_reply":"2022-05-26T13:52:18.756737Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\nsub.loc[:, \"target\"] = test_pred\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:52:18.758925Z","iopub.execute_input":"2022-05-26T13:52:18.759441Z","iopub.status.idle":"2022-05-26T13:52:19.011654Z","shell.execute_reply.started":"2022-05-26T13:52:18.759389Z","shell.execute_reply":"2022-05-26T13:52:19.010856Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def process_image(image_path):\n    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n        returns an Numpy array\n    '''\n    \n    # Process a PIL image for use in a PyTorch model\n    \n    pil_image = Image.open(image_path)\n    \n    # Resize\n    if pil_image.size[0] > pil_image.size[1]:\n        pil_image.thumbnail((5000, 256))\n    else:\n        pil_image.thumbnail((256, 5000))\n        \n    # Crop \n    left_margin = (pil_image.width-256)/2\n    bottom_margin = (pil_image.height-256)/2\n    right_margin = left_margin + 256\n    top_margin = bottom_margin + 256\n    \n    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n    \n    \n    # Normalize\n    np_image = np.array(pil_image)/255\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    np_image = (np_image - mean) / std\n  \n    # PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array\n    # Color channel needs to be first; retain the order of the other two dimensions.\n    np_image = np_image.transpose((2, 0, 1))\n    \n    return np_image\n\n","metadata":{"scrolled":true,"id":"7VlLkC4RBOC4","execution":{"iopub.status.busy":"2022-05-26T13:58:23.588612Z","iopub.execute_input":"2022-05-26T13:58:23.588972Z","iopub.status.idle":"2022-05-26T13:58:23.598872Z","shell.execute_reply.started":"2022-05-26T13:58:23.588942Z","shell.execute_reply":"2022-05-26T13:58:23.597805Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{"id":"5dS38R6wBODJ"}},{"cell_type":"code","source":"test = test_df['target']= np.zeros((len(test_df), 1))\npred = np.round(test_pred)\ncm = confusion_matrix(test, pred)\n\ncm_df = pd.DataFrame(cm,\n                     index = ['Benign','Malignant'], \n                     columns = ['Benign','Malignant'])\n\nplt.figure(figsize=(5.5,4))\nsb.heatmap(cm_df, annot=True)\nplt.title('Confusion Matrix \\nAccuracy:{0:.3f}'.format(test_accuracy))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n\nplt.savefig('Confusion Matrix02.jpg')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:53:58.384727Z","iopub.execute_input":"2022-05-26T13:53:58.385058Z","iopub.status.idle":"2022-05-26T13:53:58.626537Z","shell.execute_reply.started":"2022-05-26T13:53:58.385027Z","shell.execute_reply":"2022-05-26T13:53:58.625676Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Implement the code to predict the class from an image file\n\ndef predict(image_path, model, topk=1): #just 2 classes from 1 single output\n    ''' Predict the class (or classes) of an image using a trained deep learning model.\n    '''\n    \n    image = process_image(image_path)\n    \n    # Convert image to PyTorch tensor first\n    image = torch.from_numpy(image).type(torch.cuda.FloatTensor)\n    #print(image.shape)\n    #print(type(image))\n    \n    # Returns a new tensor with a dimension of size one inserted at the specified position.\n    image = image.unsqueeze(0)\n    \n    output = model(image)\n    \n    probabilities = torch.sigmoid(output)\n    \n    # Probabilities and the indices of those probabilities corresponding to the classes\n    top_probabilities, top_indices = probabilities.topk(topk)\n    \n    # Convert to lists\n    top_probabilities = top_probabilities.detach().type(torch.FloatTensor).numpy().tolist()[0] \n    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0] \n    \n    top_classes = []\n    \n    if probabilities > 0.5 :\n        top_classes.append(\"Melanoma\")\n    else:\n        top_classes.append(\"Benign\")\n\n    \n    return top_probabilities, top_classes\n\npredict_image_path='../input/siim-isic-melanoma-classification/jpeg/train/ISIC_0502582.jpg'\n#predict_image_path='../input/siim-isic-melanoma-classification/jpeg/test/ISIC_0074618.jpg'\n\nprobs, classes = predict(predict_image_path, model)   \nprint(probs)\nprint(classes)","metadata":{"id":"gnR_JjzSBODE","execution":{"iopub.status.busy":"2022-05-26T13:59:56.930135Z","iopub.execute_input":"2022-05-26T13:59:56.930481Z","iopub.status.idle":"2022-05-26T13:59:57.403908Z","shell.execute_reply.started":"2022-05-26T13:59:56.930449Z","shell.execute_reply":"2022-05-26T13:59:57.403145Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Display an image along with the diagnosis of melanoma or benign\n\n# Plot Skin image input image\nplt.figure(figsize = (6,10))\nplot_1 = plt.subplot(2,1,1)\n\nimage = process_image(predict_image_path)\n\nimshow(image, plot_1)\nfont = {\"color\": 'g'} if 'Benign' in classes else {\"color\": 'r'}\nplot_1.set_title(\"Diagnosis: {}\".format(classes), fontdict=font);","metadata":{"id":"KO7XaIeiBODL","execution":{"iopub.status.busy":"2022-05-26T14:00:15.766010Z","iopub.execute_input":"2022-05-26T14:00:15.766458Z","iopub.status.idle":"2022-05-26T14:00:15.969292Z","shell.execute_reply.started":"2022-05-26T14:00:15.766395Z","shell.execute_reply":"2022-05-26T14:00:15.968279Z"},"trusted":true},"execution_count":38,"outputs":[]}]}